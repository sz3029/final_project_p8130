---
title: "Final Project"
author: "Group 16"
date: "12/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Preparation and EDA

```{r, message = FALSE}
library(dplyr)
library(MASS)
library(ggplot2)
library(GGally)
library(leaps)
library(tidyverse)
library(performance)
library(patchwork)
library(interactions)
library(modelr)
library(caret)

crime_df = 
  read_csv("cdi.csv") %>% 
  mutate(
    region = factor(region),
    pop_density = pop / area,
    beds_density_1000 =1000 * beds / pop,
    docs_density_1000 =1000 * docs / pop,
    crm_1000 = 1000 * crimes / pop
  ) %>% 
  dplyr::select(-crimes, -id,-area, -pop, -beds, -docs) %>% 
  dplyr::select(crm_1000, everything()) %>%
  drop_na()
```

## Independent Variable Selection

First examine the marginal distributions of each variable.

```{r, message = FALSE}
#Load data
crime_df1 = 
  read_csv("cdi.csv") %>% 
  mutate(
    region = factor(region),
    pop_density = pop / area,
    crm_1000 = 1000 * crimes / pop
  ) %>%
  dplyr::select(-pop, -id, -cty, -state, -area, -crimes)
```

To begin with, we take a look on the distribution of crime rate per 1000 population. We find that Kings County in NY is an extreme outlier with close to 300 crime rate, so we may not include this data in the following analysis.

First examine the marginal distributions of each variable.

```{r plots, message=FALSE}
# graph
compute_plist = function(x) {
  if (is.factor(x)) {
    pl = crime_df1 %>% 
    ggplot(aes(x = x, y = crm_1000)) +
    geom_boxplot() +
    labs(
      x = '',
      y = "Crime Rate",
    )
  } else {
    pl = crime_df1 %>% 
    ggplot(aes(x = x, y = crm_1000)) +
    geom_point() +
    geom_smooth(method = "lm") +
    labs(
      x = '',
      y = "Crime Rate",
    )
  }
  return(pl)
}

plist = crime_df1 %>%
  dplyr::select(-crm_1000) %>%
  map(compute_plist)

labels = crime_df1 %>%
  dplyr::select(-crm_1000) %>%
  names()

for (i in 1:length(plist)) {
  plist[[i]] = plist[[i]] + labs(x = labels[[i]])
}

wrap_plots(plist, ncol = 4)
```

Compare p-value of all variables

```{r}
compute_p_value = function(x) {
  lm_data = lm(crm_1000 ~ x, data = crime_df1)
  if (is.factor(x)) {
    return( lm_data %>% broom::tidy() %>% dplyr::select(p.value) %>% .[2:4,1])
  }
  return(lm_data %>% broom::tidy() %>% dplyr::select(p.value) %>% .[2,1])
}

labels_list <- c("pop18","pop65","docs","beds","hsgrad","bagrad","poverty","unemp","pcincome","totalinc","region_1","region_2", "region_3", "pop_density")

p_values = crime_df1 %>%
  dplyr::select(-crm_1000) %>%
  map_df(compute_p_value) %>%
  mutate(Variables = labels_list) 

p_values
```

### Backward & Forward & Stepwise
```{r}
crime_df2 = 
  read_csv("cdi.csv") %>% 
  mutate(
    region = factor(region),
    pop_density = pop / area,
    crm_1000 = 1000 * crimes / pop,
    beds_density_1000 = 1000 * beds / pop,
    docs_density_1000 = 1000 * docs / pop
  ) %>% 
  dplyr::select(-crimes, -id, -area, -pop, -cty, -state, -beds, -docs, -region) %>%
  dplyr::select(crm_1000, everything())
# fit regression using all predictors
mult.fit = lm(crm_1000 ~ ., data = crime_df)

step(mult.fit, direction = 'backward')

step(mult.fit, direction = 'forward')

step(mult.fit, direction = 'both')
```

## Test Based Procedures

```{r}
mat = as.matrix(crime_df2)
# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = mat[,2:12], y = mat[,1], nbest = 2, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = mat[,2:12], y = mat[,1], nbest = 2, method = "adjr2")

# Function regsubsets() performs a subset selection by identifying the "best" model that contains
# a certain number of predictors. By default "best" is chosen using SSE/RSS (smaller is better)
b = regsubsets(crm_1000 ~ ., data = crime_df2)
rs = summary(b)

# plot of Cp and Adj-R2 as functions of parameters
par(mfrow=c(1,2))

plot(2:9, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)
#6
plot(2:9, rs$adjr2, xlab="No of parameters", ylab="Adj R2")
#6
```

```{r}
Model_backward =
lm(formula = crm_1000 ~ pop18 + hsgrad + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000, data = crime_df)
summary(Model_backward)
BIC(Model_backward)

Model_forward =
  lm(formula = crm_1000 ~ pop18 + pop65 + hsgrad + bagrad + poverty + unemp + pcincome + totalinc + region + pop_density + beds_density_1000 + docs_density_1000, data = crime_df)
BIC(Model_forward)
summary(Model_forward)

Model_stepwise = 
  lm(formula = crm_1000 ~ pop18 + hsgrad + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000, data = crime_df)
summary(Model_stepwise)
BIC(Model_stepwise)

cp1 = 
  lm(formula = crm_1000 ~ pop18 + hsgrad + poverty + unemp + totalinc + pop_density + docs_density_1000, data = crime_df2)
BIC(cp1)
summary(cp1)

cp2 = 
  lm(formula = crm_1000 ~ pop65 + hsgrad + poverty + unemp + totalinc + pop_density + docs_density_1000, data = crime_df2)
BIC(cp2)
summary(cp2)
```

### select based on marginal distribution
```{r}
#linear regression model with full predictors
fit_no_inte = lm(crm_1000 ~ pop18 +totalinc + poverty + hsgrad + beds_density_1000 + docs_density_1000 + region + pop_density, data = crime_df)
summary(fit_no_inte)
```

```{r}
fit_no = lm(crm_1000 ~ pop18 +totalinc + poverty + hsgrad + beds_density_1000 + docs_density_1000 + region + pop_density + pcincome, data = crime_df)
small = lm(crm_1000 ~ pop18 +totalinc + poverty + hsgrad + beds_density_1000 + docs_density_1000 + region + pop_density , data = crime_df)

anova(small, fit_no)
```

Next using partial ANOVA to test whether large model is superior

no pop_density
```{r}
fit_1 = lm(crm_1000 ~ pop18 + totalinc + poverty + hsgrad + beds_density_1000 + docs_density_1000 + region, data = crime_df)
summary(fit_1)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_1, fit_no_inte)
```

Reject H0, large model is superior

### no region

```{r}
fit_2 = lm(crm_1000 ~ pop18 + totalinc + poverty + hsgrad + beds_density_1000 + docs_density_1000 + pop_density, data = crime_df)
summary(fit_2)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_2, fit_no_inte)
```

Reject H0, large model is superior


no docs_density_1000
```{r}
fit_3 = lm(crm_1000 ~  pop18 + totalinc + poverty + hsgrad + beds_density_1000 + region + pop_density, data = crime_df)
summary(fit_3)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_3, fit_no_inte)
```

Fail to reject H0, large model is not superior, we should keep the smaller one.

no beds_density
```{r}
fit_4 = lm(crm_1000 ~ pop18 + totalinc + poverty + hsgrad + region + pop_density + docs_density_1000, data = crime_df)
summary(fit_4)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_4, fit_no_inte)
```

Reject H0, large model is superior

no hsgrad
```{r}
fit_5 = lm(crm_1000 ~ pop18 + totalinc + poverty + region + pop_density + docs_density_1000 + beds_density_1000, data = crime_df)
summary(fit_5)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_5, fit_no_inte)
```

Fail to reject H0, large model is not superior, we should keep the smaller one.

no poverty
```{r}
fit_6 = lm(crm_1000 ~ pop18 + totalinc + region + pop_density + docs_density_1000 + beds_density_1000 + hsgrad, data = crime_df)
summary(fit_6)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_6, fit_no_inte)
```

Reject H0, large model is superior

no totalinc
```{r}
fit_7 = lm(crm_1000 ~ pop18 + poverty + hsgrad + beds_density_1000 + docs_density_1000 + region + pop_density, data = crime_df)
summary(fit_7)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_7, fit_no_inte)
```

Reject H0, large model is superior.

no pop18
```{r}
fit_8 = lm(crm_1000 ~ poverty + hsgrad + beds_density_1000 + docs_density_1000 + region + pop_density + totalinc, data = crime_df)
summary(fit_7)
# compare nested (small vs large) models
# Ho: smaller model is defensible
anova(fit_8, fit_no_inte)
```

Fail to reject H0, large model is not superior, we should keep the smaller one.

## Interations
### Model 1
```{r}
fiti <- lm(crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density, data = crime_df)
summary(fiti)
interact_plot(fiti, pred = pop18, modx = pop_density)
```

### Backward Model
```{r cars}
fiti <- lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + 
    totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density, data = crime_df)
summary(fiti)
interact_plot(fiti, pred = bagrad, modx = pcincome)
interact_plot(fiti, pred = bagrad, modx = pop_density)
```

## Candidate models

Thus the four candidate regression model should be:
```{r}
model_1 = lm(crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density, data = crime_df)

model_2 = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000, data = crime_df)

model_3 = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density, data = crime_df)
```

## Model Diagnostics

### Boxcox Transformation

Model1
```{r}
bc <- boxcox(model_1, lambda = seq(-5, 5, by = 0.25))
lambda <- bc$x[which.max(bc$y)]
#choose power 1/2

crime_df$crm.bc <- crime_df$crm_1000^(1/2)
model_bc1 = lm(crm.bc ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density, data = crime_df)

par(mfrow = c(1,2))
plot(model_1, which = 2)
plot(model_bc1, which = 2)

summary(model_1)
summary(model_bc1)
```

Model2
```{r}
bc <- boxcox(model_2, lambda = seq(-5, 5, by = 0.25))
lambda <- bc$x[which.max(bc$y)]
#choose power 1/2

crime_df$crm.bc <- crime_df$crm_1000^(1/2)
model_bc2 = lm(crm.bc ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000, data = crime_df)

par(mfrow = c(1,2))
plot(model_2, which = 2)
plot(model_bc2, which = 2)

summary(model_2)
summary(model_bc2)
```

Model3
```{r}
bc <- boxcox(model_3, lambda = seq(-5, 5, by = 0.25))
lambda <- bc$x[which.max(bc$y)]
#choose power 1/2

crime_df$crm.bc <- crime_df$crm_1000^(1/2)
model_bc3 = lm(crm.bc ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density, data = crime_df)

par(mfrow = c(1,2))
plot(model_3, which = 2)
plot(model_bc3, which = 2)

summary(model_3)
summary(model_bc3)
```

### Residuals 

```{r}
plot(model_1, which = 1)
plot(model_2, which = 1)
plot(model_3, which = 1)
```
observation #6, #215

### Normal QQ plots

Filter out influential observations

```{r backward}
plot(model_1, which = 2)
plot(model_2, which = 2)
plot(model_3, which = 2)
```

The observation #6, #215, #123is an outlier in all three

### Scale and locations

```{r}
plot(model_1, which = 3)
plot(model_2, which = 3)
plot(model_3, which = 3)
```
The observation #123, #6, #215 is identified

### Outliers and Leverage
```{r}
plot(model_1, which = 5)
plot(model_2, which = 5)
plot(model_3, which = 5)
```

observation #1, #6

```{r}
plot(model_1, which = 6)
plot(model_2, which = 6)
plot(model_3, which = 6)
```
 #1, #123, #6

The observation #6 and #1, is beyond the Cook’s distance lines of 0.5 (> 1). The plot identified the influential observation as #6, #1

```{r backward_adj}
crime_df %>%
  filter(row(crime_df) == 6 | row(crime_df) == 215 )
crime_after <- crime_df %>%
  filter(row(crime_df) != 6)
```

### Colinearility

```{r}
# Correlation matrix for all variables
temp <- crime_after %>%
  dplyr::select(-state, -cty, -region)

cor(temp)
```

```{r colinear}
crime_after %>% 
  dplyr::select(-crm_1000) %>% 
  ggcorr(label = TRUE, label_size = 2, hjust = 0.8)
```

The correlation plot suggest high correlation between beds and docs, beds and totalinc, docs and totalinc. 

Let's check if the model violates colienarity

Calculate VIF w/o interaction terms
```{r}
model_1_new = lm(crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density, data = crime_after)

model_2_new = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + 
    totalinc + region + pop_density + beds_density_1000, data = crime_after)

model_3_new = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + 
    totalinc + region + pop_density + beds_density_1000, data = crime_after)

check_collinearity(model_1_new)

check_collinearity(model_2_new)

check_collinearity(model_3_new)
```

There is no multicolinearity in our models

## Final models

```{r}
model_1 = lm(crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density, data = crime_after)

model_2 = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + 
    totalinc + region + pop_density + beds_density_1000, data = crime_after)

model_3 = lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + 
    totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density, data = crime_after)

model_1 %>% broom::tidy()
model_2 %>% broom::tidy()
model_3 %>% broom::tidy()
```

## Cross validation
```{r}
set.seed(7)

cv_df =
  crossv_mc(crime_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    model_1 = map(train, ~lm(crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density, data = .x)),
    model_2 = map(train, ~lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000, data = .x)),
    model_3 = map(train, ~lm(crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density, data = .x))
  ) %>% 
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))
  )
  
cv_df

cv_df %>% 
  dplyr::select(rmse_1:rmse_3) %>% 
  pivot_longer(
    rmse_1:rmse_3,
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
  labs(
    x = "Model",
    y = "RMSE"
  )
```

```{r}
set.seed(2021)

# Use 5-fold validation and create the training sets
train = trainControl(method = "repeatedcv", number = 5, repeats = 100)

# Fit the 4-variables model that we discussed in previous lectures
model_caret_1 = 
  train(
    crm_1000 ~ pop18 +totalinc + poverty + beds_density_1000 + region + pop_density + pop18*pop_density,
    data = crime_df,
    trControl = train,
    method = 'lm',
    na.action = na.pass
  )

model_caret_2 = 
  train(
    crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000,
    data = crime_df,
    trControl = train,
    method = 'lm',
    na.action = na.pass
  )

model_caret_3 = 
  train(
    crm_1000 ~ pop18 + bagrad + poverty + pcincome + totalinc + region + pop_density + beds_density_1000 + bagrad*pcincome + bagrad*pop_density,
    data = crime_df,
    trControl = train,
    method = 'lm',
    na.action = na.pass
  )

bind_rows(
  as_tibble(model_caret_1$results),
  as_tibble(model_caret_2$results),
  as_tibble(model_caret_3$results),
) %>% 
  knitr::kable(digit = 3)
```



